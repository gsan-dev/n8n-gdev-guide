# 3.1 Instalación mediante Docker (Recomendado para Producción)

Para la implementación eficiente y escalable del sistema **GAR (Generador Automatizado de Respuestas)**, recomendamos el uso de contenedores Docker. Este método simplifica el despliegue de múltiples servicios de inteligencia artificial local, permitiendo una puesta en marcha rápida, confiable y reproducible.

## Herramientas involucradas

A continuación, se describen las herramientas fundamentales que componen esta arquitectura modular basada en contenedores:

- **Docker**: Plataforma de contenedorización que encapsula los servicios y sus dependencias en entornos aislados. Facilita la gestión, despliegue y replicabilidad del entorno de IA con un único comando.
- **n8n**: Entorno de automatización de flujos de trabajo de código abierto, ideal para construir integraciones complejas de IA sin necesidad de escribir código. Proporciona una interfaz gráfica intuitiva.
- **PostgreSQL**: Sistema de gestión de bases de datos relacional que actúa como almacenamiento persistente para n8n.
- **Qdrant**: Motor de búsqueda vectorial optimizado para contenido generado por IA. Facilita búsquedas semánticas y almacenamiento de vectores.
- **Ollama**: Plataforma de despliegue local de modelos de lenguaje, que permite ejecutar LLMs de código abierto con requisitos mínimos de hardware.

El marco de automatización principal es **n8n**, que orquesta los flujos de trabajo inteligentes del chatbot RAG. **Qdrant** se encarga de almacenar vectores semánticos y **Ollama** opera como servidor de modelos de IA. En conjunto, estos servicios conforman la columna vertebral del sistema GAR.

## Requisitos previos

1. Instalar **Docker** desde el sitio oficial: [https://www.docker.com](https://www.docker.com)
2. Usuarios de **Windows** deberán habilitar **WSL (Windows Subsystem for Linux)** para ejecutar contenedores Docker. Ejecute el siguiente comando desde PowerShell (con privilegios de administrador):

```powershell
wsl --install
```

3. Una vez instalado WSL, reinicie el sistema. Verifique que Docker se ha instalado correctamente ejecutando:

```bash
docker run hello-world
```

Este comando descargará y ejecutará un contenedor de prueba. Si recibe el mensaje de éxito, Docker está funcionando correctamente.

## Instalación y despliegue de servicios IA con Docker Compose

Utilizaremos **Docker Compose** para levantar toda la infraestructura local de servicios IA de manera eficiente.

1. Clonar el repositorio del kit de inicio:

```bash
git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git
cd self-hosted-ai-starter-kit
```

2. Iniciar los contenedores con soporte para CPU:

```bash
docker compose --profile cpu up
```

3. Si se dispone de una **GPU NVIDIA**, es recomendable ejecutar el siguiente perfil para aprovechar la aceleración por hardware (tras haber configurado Docker para usar GPU):

```bash
docker compose --profile gpu-nvidia up
```

> **Nota**: La descarga inicial de las imágenes y el arranque de los contenedores puede tardar varios minutos.

## Verificación de estado de los contenedores

Para comprobar que todos los servicios están activos, ejecute:

```bash
docker compose ps
```

Se mostrará el estado actual de cada contenedor y su disponibilidad.

## Descarga de modelos necesarios

El kit inicial descarga automáticamente el modelo Llama 3.2. Para completar el entorno de un sistema **RAG (Retrieval-Augmented Generation)**, se debe añadir también un modelo de embeddings. Para ello, desde el contenedor de Ollama, ejecute:

```bash
ollama pull nomic-embed-text
```

Este modelo permitirá realizar comparaciones semánticas y recuperar contenido relevante en las respuestas del chatbot.

## Acceso al panel de n8n

Una vez en ejecución, acceda al entorno gráfico de n8n desde el navegador:

```
http://localhost:5678
```

Configure una cuenta de usuario con su correo electrónico y una contraseña. Luego, acceda al flujo de trabajo de demostración disponible en el panel principal.

---

Este entorno modular desplegado con Docker permite operar todos los componentes de IA de forma conjunta, estable y escalable. Es ideal tanto para pruebas locales como para entornos de producción controlados.

---

<div align="center" style="border: 1px solid #4F8AFA; border-radius: 12px; padding: 20px; background: #f0f6ff; margin-top: 32px; display: flex; justify-content: center; gap: 32px;">
  <a href="3.0.%20Metodos%20de%20instalacion%20Local.md" style="text-decoration:none; font-weight: bold; color: #4F8AFA; font-size: 1.1em;">⬅️ Anterior: 3.0 Métodos de Instalación Local</a>
  <a href="../Configuracion-Inicial/4.1.%20Acceso%20al%20panel%20de%20administración.md" style="text-decoration:none; font-weight: bold; color: #4F8AFA; font-size: 1.1em;">➡️ Siguiente: 4.1 Acceso al panel de administración</a>
</div>


